"""
Scraper de base avec logique commune
"""
import asyncio
import json
import random
from abc import ABC
from pathlib import Path
from typing import List, Optional

from jobspy import scrape_jobs
from france_chomage.config import settings
from france_chomage.environments import get_sites_for_environment, is_docker
from france_chomage.models import Job
from france_chomage.database import job_manager

class ScraperBase(ABC):
    """Classe de base pour tous les scrapers"""
    
    # Ã€ dÃ©finir dans les sous-classes
    search_terms: str
    filename_prefix: str
    job_type: str  # Pour les hashtags
    
    def __init__(self):
        pass
    
    async def scrape(self) -> List[Job]:
        """Point d'entrÃ©e principal pour scraper"""
        print(f"ğŸ” DÃ©but du scraping {self.job_type}")
        
        jobs = await self._scrape_with_retry()
        
        if jobs:
            print(f"âœ… Scraping terminÃ© - {len(jobs)} offres trouvÃ©es")
            
            # Save to database with filtering and deduplication
            await self._save_to_database(jobs)
            
            # Keep JSON backup for compatibility
            self._save_jobs(jobs)
        else:
            print("âš ï¸ Aucune offre trouvÃ©e")
            self._save_empty_file()
            
        return jobs or []
    
    async def _scrape_with_retry(self) -> Optional[List[Job]]:
        """Scrape avec logique de retry"""
        sites = get_sites_for_environment()
        env_type = 'Docker' if is_docker() else 'Local'
        print(f"ğŸŒ Sites: {', '.join(sites)} ({env_type})")
        
        if 'indeed' in sites:
            print("âš ï¸ Indeed inclus - Risque de blocage 403 Ã©levÃ©")
        if env_type == 'Local' and len(sites) > 1:
            print("ğŸ  Mode Local dÃ©tectÃ© - Indeed + LinkedIn (fallback automatique)")
        elif env_type == 'Docker':
            print("ğŸ³ Mode Docker dÃ©tectÃ© - Indeed + LinkedIn (fallback automatique)")
        
        for attempt in range(1, settings.max_retries + 1):
            try:
                print(f"ğŸ”„ Tentative {attempt}/{settings.max_retries}")
                
                # DÃ©lai alÃ©atoire anti-dÃ©tection (plus long pour Indeed)
                if 'indeed' in sites and attempt > 1:
                    # DÃ©lais plus longs aprÃ¨s un Ã©chec avec Indeed
                    delay = random.uniform(5.0, 15.0)
                    print(f"â³ DÃ©lai anti-Indeed: {delay:.1f}s")
                else:
                    delay = random.uniform(settings.scrape_delay_min, settings.scrape_delay_max)
                    print(f"â³ Attente standard: {delay:.1f}s")
                await asyncio.sleep(delay)
                
                # ParamÃ¨tres de scraping avec stratÃ©gies anti-dÃ©tection
                scrape_params = {
                    'site_name': list(sites),
                    'search_term': self.search_terms,
                    'location': settings.location,
                    'results_wanted': min(settings.results_wanted, 10) if 'indeed' in sites else settings.results_wanted,
                    'country_indeed': settings.country,

                }
                
                # RÃ©duction du nombre de rÃ©sultats pour Indeed
                if 'indeed' in sites:
                    scrape_params['results_wanted'] = min(scrape_params['results_wanted'], settings.indeed_max_results)
                    print(f"ğŸ¯ Limitation Indeed: max {settings.indeed_max_results} rÃ©sultats pour Ã©viter la dÃ©tection")
                
                print(f"ğŸ“ Recherche: '{self.search_terms}' Ã  {settings.location} ({settings.results_wanted} rÃ©sultats)")
                print(f"ğŸŒ Sites ciblÃ©s: {', '.join(scrape_params['site_name'])}")
                print(f"ğŸ”§ ParamÃ¨tres complets: {scrape_params}")
                
                # Scraping synchrone (jobspy n'est pas async)
                print("ğŸš€ Lancement de jobspy...")
                df = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: scrape_jobs(**scrape_params)
                )
                print("ğŸ“Š RÃ©ponse jobspy reÃ§ue")
                
                if df is not None and len(df) > 0:
                    print(f"ğŸ“„ DataFrame reÃ§u: {len(df)} lignes, colonnes: {list(df.columns)}")
                    jobs = self._dataframe_to_jobs(df)
                    print(f"ğŸ‰ SuccÃ¨s! {len(jobs)} offres rÃ©cupÃ©rÃ©es aprÃ¨s parsing")
                    return jobs
                else:
                    if df is None:
                        print(f"âš ï¸ DataFrame vide (None) - tentative {attempt}")
                    else:
                        print(f"âš ï¸ DataFrame sans donnÃ©es ({len(df)} lignes) - tentative {attempt}")
                    
            except Exception as exc:
                error_msg = str(exc)
                print(f"âŒ Erreur tentative {attempt}: {error_msg}")
                
                # DÃ©tection des erreurs spÃ©cifiques
                if "403" in error_msg:
                    print("ğŸš« Erreur 403 dÃ©tectÃ©e - Blocage anti-bot probable")
                    
                    # Fallback automatique vers LinkedIn uniquement
                    if 'indeed' in sites and 'linkedin' in sites and len(sites) > 1:
                        print("ğŸ”„ Fallback automatique: tentative avec LinkedIn uniquement...")
                        linkedin_only_params = scrape_params.copy()
                        linkedin_only_params['site_name'] = ['linkedin']
                        linkedin_only_params['results_wanted'] = settings.results_wanted  # Pas de limite pour LinkedIn
                        
                        try:
                            print("ğŸ”— Tentative LinkedIn seul...")
                            df_linkedin = await asyncio.get_event_loop().run_in_executor(
                                None, lambda: scrape_jobs(**linkedin_only_params)
                            )
                            
                            if df_linkedin is not None and len(df_linkedin) > 0:
                                print(f"âœ… SuccÃ¨s LinkedIn! {len(df_linkedin)} offres trouvÃ©es")
                                jobs = self._dataframe_to_jobs(df_linkedin)
                                return jobs
                            else:
                                print("âš ï¸ LinkedIn n'a pas retournÃ© de rÃ©sultats")
                        except Exception as linkedin_exc:
                            print(f"âŒ Ã‰chec LinkedIn aussi: {linkedin_exc}")
                    
                    print("ğŸ’¡ Suggestions pour Ã©viter les 403:")
                    print("   â€¢ Indeed bloque souvent les scrapers")
                    print("   â€¢ Utilisez DOCKER=1 pour LinkedIn uniquement")
                    print("   â€¢ RÃ©duisez RESULTS_WANTED Ã  5-10")
                    print("   â€¢ Augmentez les dÃ©lais entre scraping")
                elif "timeout" in error_msg.lower():
                    print("â° Timeout dÃ©tectÃ© - Connexion lente ou serveur surchargÃ©")
                elif "connection" in error_msg.lower():
                    print("ğŸŒ ProblÃ¨me de connexion rÃ©seau")
                
                print(f"ğŸ” Type d'erreur: {type(exc).__name__}")
                print(f"ğŸ“ Message complet: {repr(exc)}")
                
                if attempt < settings.max_retries:
                    wait_time = settings.retry_delay_base * attempt
                    print(f"ğŸ”„ Attente {wait_time}s avant nouvelle tentative...")
                    await asyncio.sleep(wait_time)
                else:
                    print("ğŸ’¥ Toutes les tentatives Ã©puisÃ©es")
        
        print("ğŸ’¥ Toutes les tentatives ont Ã©chouÃ©")
        return None
    
    def _dataframe_to_jobs(self, df) -> List[Job]:
        """Convertit le DataFrame pandas en liste de Jobs"""
        jobs = []
        for _, row in df.iterrows():
            try:
                job_data = row.to_dict()
                # Nettoie les valeurs NaN/None
                job_data = {k: v for k, v in job_data.items() if v is not None and str(v) != 'nan'}
                
                # Convertit date_posted en string si c'est un objet date/datetime
                if 'date_posted' in job_data and hasattr(job_data['date_posted'], 'strftime'):
                    job_data['date_posted'] = job_data['date_posted'].strftime('%Y-%m-%d')
                
                # Ajoute une date par dÃ©faut si manquante
                if 'date_posted' not in job_data:
                    from datetime import date
                    job_data['date_posted'] = date.today().strftime('%Y-%m-%d')
                
                job = Job(**job_data)
                jobs.append(job)
            except Exception as exc:
                print(f"âš ï¸ Erreur parsing job: {str(exc)}")
                continue
        
        return jobs
    
    async def _save_to_database(self, jobs: List[Job]) -> None:
        """Save jobs to database with filtering and deduplication"""
        try:
            new_jobs, filtered_count = await job_manager.process_scraped_jobs(
                jobs=jobs,
                category=self.job_type,
                max_age_days=30  # Only jobs from last 30 days
            )
            
            if new_jobs:
                print(f"ğŸ’¾ Database: {len(new_jobs)} nouveaux jobs sauvegardÃ©s")
            if filtered_count > 0:
                print(f"ğŸ” Database: {filtered_count} jobs filtrÃ©s (anciens/doublons)")
                
        except Exception as exc:
            print(f"âŒ Erreur sauvegarde database: {exc}")
            print("ğŸ“„ Continuons avec la sauvegarde JSON...")
    
    def _save_jobs(self, jobs: List[Job]) -> None:
        """Sauvegarde les jobs en JSON"""
        output_path = Path(f"jobs_{self.filename_prefix}.json")
        
        jobs_data = [job.model_dump() for job in jobs]
        
        with output_path.open('w', encoding='utf-8') as f:
            json.dump(jobs_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ {len(jobs)} jobs sauvegardÃ©s dans {output_path}")
    
    def _save_empty_file(self) -> None:
        """Sauvegarde un fichier vide en cas d'Ã©chec (backup compatibility)"""
        output_path = Path(f"jobs_{self.filename_prefix}.json")
        
        with output_path.open('w', encoding='utf-8') as f:
            json.dump([], f)
        
        print(f"ğŸ“„ Fichier backup vide crÃ©Ã©: {output_path}")
