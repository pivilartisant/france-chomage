"""
Scraper de base avec logique commune
"""
import asyncio
import json
import random
from abc import ABC
from pathlib import Path
from typing import List, Optional

from jobspy import scrape_jobs
from france_chomage.config import settings
from france_chomage.environments import get_sites_for_environment, is_docker
from france_chomage.models import Job
from france_chomage.database import job_manager

class ScraperBase(ABC):
    """Classe de base pour tous les scrapers"""
    
    # √Ä d√©finir dans les sous-classes
    search_terms: str
    filename_prefix: str
    job_type: str  # Pour les hashtags
    
    def __init__(self):
        pass
    
    async def scrape(self) -> List[Job]:
        """Point d'entr√©e principal pour scraper"""
        print(f"üîç D√©but du scraping {self.job_type}")
        
        jobs = await self._scrape_with_retry()
        
        if jobs:
            print(f"‚úÖ Scraping termin√© - {len(jobs)} offres trouv√©es")
            
            # Save to database with filtering and deduplication
            await self._save_to_database(jobs)
            
            # Keep JSON backup for compatibility
            self._save_jobs(jobs)
        else:
            print("‚ö†Ô∏è Aucune offre trouv√©e")
            self._save_empty_file()
            
        return jobs or []
    
    async def _scrape_with_retry(self) -> Optional[List[Job]]:
        """Scrape avec logique de retry"""
        sites = get_sites_for_environment()
        env_type = 'Docker' if is_docker() else 'Local'
        print(f"üåê Sites: {', '.join(sites)} ({env_type})")
        
        if 'indeed' in sites:
            print("‚ö†Ô∏è Indeed inclus - Risque de blocage 403 √©lev√©")
        if env_type == 'Local' and len(sites) > 1:
            print("üè† Mode Local d√©tect√© - Indeed + LinkedIn (fallback automatique)")
        elif env_type == 'Docker':
            print("üê≥ Mode Docker d√©tect√© - Indeed + LinkedIn (fallback automatique)")
        
        for attempt in range(1, settings.max_retries + 1):
            try:
                print(f"üîÑ Tentative {attempt}/{settings.max_retries}")
                
                # D√©lai al√©atoire anti-d√©tection (plus long pour Indeed)
                if 'indeed' in sites and attempt > 1:
                    # D√©lais plus longs apr√®s un √©chec avec Indeed
                    delay = random.uniform(5.0, 15.0)
                    print(f"‚è≥ D√©lai anti-Indeed: {delay:.1f}s")
                else:
                    delay = random.uniform(settings.scrape_delay_min, settings.scrape_delay_max)
                    print(f"‚è≥ Attente standard: {delay:.1f}s")
                await asyncio.sleep(delay)
                
                # Param√®tres de scraping avec strat√©gies anti-d√©tection
                scrape_params = {
                    'site_name': list(sites),
                    'search_term': self.search_terms,
                    'location': settings.location,
                    'results_wanted': min(settings.results_wanted, 10) if 'indeed' in sites else settings.results_wanted,
                    'country_indeed': settings.country,

                }
                
                # R√©duction du nombre de r√©sultats pour Indeed
                if 'indeed' in sites:
                    scrape_params['results_wanted'] = min(scrape_params['results_wanted'], settings.indeed_max_results)
                    print(f"üéØ Limitation Indeed: max {settings.indeed_max_results} r√©sultats pour √©viter la d√©tection")
                
                print(f"üìç Recherche: '{self.search_terms}' √† {settings.location} ({settings.results_wanted} r√©sultats)")
                print(f"üåê Sites cibl√©s: {', '.join(scrape_params['site_name'])}")
                print(f"üîß Param√®tres complets: {scrape_params}")
                
                # Scraping synchrone (jobspy n'est pas async)
                print("üöÄ Lancement de jobspy...")
                df = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: scrape_jobs(**scrape_params)
                )
                print("üìä R√©ponse jobspy re√ßue")
                
                if df is not None and len(df) > 0:
                    print(f"üìÑ DataFrame re√ßu: {len(df)} lignes, colonnes: {list(df.columns)}")
                    jobs = self._dataframe_to_jobs(df)
                    print(f"üéâ Succ√®s! {len(jobs)} offres r√©cup√©r√©es apr√®s parsing")
                    return jobs
                else:
                    if df is None:
                        print(f"‚ö†Ô∏è DataFrame vide (None) - tentative {attempt}")
                    else:
                        print(f"‚ö†Ô∏è DataFrame sans donn√©es ({len(df)} lignes) - tentative {attempt}")
                    
            except Exception as exc:
                error_msg = str(exc)
                print(f"‚ùå Erreur tentative {attempt}: {error_msg}")
                
                # D√©tection des erreurs sp√©cifiques
                if "403" in error_msg:
                    print("üö´ Erreur 403 d√©tect√©e - Blocage anti-bot probable")
                    
                    # Fallback automatique vers LinkedIn uniquement
                    if 'indeed' in sites and 'linkedin' in sites and len(sites) > 1:
                        print("üîÑ Fallback automatique: tentative avec LinkedIn uniquement...")
                        linkedin_only_params = scrape_params.copy()
                        linkedin_only_params['site_name'] = ['linkedin']
                        linkedin_only_params['results_wanted'] = settings.results_wanted  # Pas de limite pour LinkedIn
                        
                        try:
                            print("üîó Tentative LinkedIn seul...")
                            df_linkedin = await asyncio.get_event_loop().run_in_executor(
                                None, lambda: scrape_jobs(**linkedin_only_params)
                            )
                            
                            if df_linkedin is not None and len(df_linkedin) > 0:
                                print(f"‚úÖ Succ√®s LinkedIn! {len(df_linkedin)} offres trouv√©es")
                                jobs = self._dataframe_to_jobs(df_linkedin)
                                return jobs
                            else:
                                print("‚ö†Ô∏è LinkedIn n'a pas retourn√© de r√©sultats")
                        except Exception as linkedin_exc:
                            print(f"‚ùå √âchec LinkedIn aussi: {linkedin_exc}")
                    
                    print("üí° Suggestions pour √©viter les 403:")
                    print("   ‚Ä¢ Indeed bloque souvent les scrapers")
                    print("   ‚Ä¢ Utilisez DOCKER=1 pour LinkedIn uniquement")
                    print("   ‚Ä¢ R√©duisez RESULTS_WANTED √† 5-10")
                    print("   ‚Ä¢ Augmentez les d√©lais entre scraping")
                elif "timeout" in error_msg.lower():
                    print("‚è∞ Timeout d√©tect√© - Connexion lente ou serveur surcharg√©")
                elif "connection" in error_msg.lower():
                    print("üåê Probl√®me de connexion r√©seau")
                
                print(f"üîç Type d'erreur: {type(exc).__name__}")
                print(f"üìù Message complet: {repr(exc)}")
                
                if attempt < settings.max_retries:
                    wait_time = settings.retry_delay_base * attempt
                    print(f"üîÑ Attente {wait_time}s avant nouvelle tentative...")
                    await asyncio.sleep(wait_time)
                else:
                    print("üí• Toutes les tentatives √©puis√©es")
        
        print("üí• Toutes les tentatives ont √©chou√©")
        return None
    
    def _dataframe_to_jobs(self, df) -> List[Job]:
        """Convertit le DataFrame pandas en liste de Jobs"""
        jobs = []
        for _, row in df.iterrows():
            try:
                job_data = row.to_dict()
                # Nettoie les valeurs NaN/None
                job_data = {k: v for k, v in job_data.items() if v is not None and str(v) != 'nan'}
                
                # Convertit date_posted en string si c'est un objet date/datetime
                if 'date_posted' in job_data and hasattr(job_data['date_posted'], 'strftime'):
                    job_data['date_posted'] = job_data['date_posted'].strftime('%Y-%m-%d')
                
                # Ajoute une date par d√©faut si manquante
                if 'date_posted' not in job_data:
                    from datetime import date
                    job_data['date_posted'] = date.today().strftime('%Y-%m-%d')
                
                job = Job(**job_data)
                jobs.append(job)
            except Exception as exc:
                print(f"‚ö†Ô∏è Erreur parsing job: {str(exc)}")
                continue
        
        return jobs
    
    async def _save_to_database(self, jobs: List[Job]) -> None:
        """Save jobs to database with filtering and deduplication"""
        try:
            new_jobs, filtered_count = await job_manager.process_scraped_jobs(
                jobs=jobs,
                category=self.job_type,
                max_age_days=30  # Only jobs from last 30 days
            )
            
            if new_jobs:
                print(f"üíæ Database: {len(new_jobs)} nouveaux jobs sauvegard√©s")
            if filtered_count > 0:
                print(f"üîç Database: {filtered_count} jobs filtr√©s (anciens/doublons)")
                
        except Exception as exc:
            print(f"‚ùå Erreur sauvegarde database: {exc}")
            print("üìÑ Continuons avec la sauvegarde JSON...")
    
    def _save_jobs(self, jobs: List[Job]) -> None:
        """Sauvegarde les jobs en JSON"""
        output_path = Path(f"jobs_{self.filename_prefix}.json")
        
        jobs_data = [job.model_dump() for job in jobs]
        
        with output_path.open('w', encoding='utf-8') as f:
            json.dump(jobs_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ {len(jobs)} jobs sauvegard√©s dans {output_path}")
    
    def _save_empty_file(self) -> None:
        """Sauvegarde un fichier vide en cas d'√©chec (backup compatibility)"""
        output_path = Path(f"jobs_{self.filename_prefix}.json")
        
        with output_path.open('w', encoding='utf-8') as f:
            json.dump([], f)
        
        print(f"üìÑ Fichier backup vide cr√©√©: {output_path}")
